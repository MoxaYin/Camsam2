"""
SAV-test æ•°æ®é›†æ¨ç†è„šæœ¬ï¼ˆSAM2+IOF é…ç½®ç‰ˆæœ¬ï¼‰

åŠŸèƒ½ç‰¹æ€§ï¼š
1. åŠ è½½è®­ç»ƒå¥½çš„ä¼ªè£…å‚æ•°
2. å†»ç»“ CamSAM2 æ–°å¢çš„ä¼ªè£…æ¨¡å—ï¼ˆEOFã€OPGã€ä¼ªè£… tokenï¼‰
3. ä¿æŒ SAM2 ä¸»å¹²å’Œ IOF æ¨¡å—å®Œå…¨æ¿€æ´»
4. ä½¿ç”¨ combined_mask è¾“å‡ºæ¨¡å¼ï¼ˆåªä½¿ç”¨ IOFï¼Œä¸ä½¿ç”¨ä¼ªè£…æŠ‘åˆ¶ï¼‰
5. ä¿å­˜åˆ†å‰²ç»“æœä¸º PNG æ ¼å¼ï¼Œä¸è¿›è¡ŒæŒ‡æ ‡è¯„ä¼°

è¿™æ˜¯ SAM2 + IOF é…ç½®ï¼Œç”¨äºåœ¨ SAV-test ä¸Šè¿›è¡Œæ¨ç†å’Œç»“æœä¿å­˜

æ•°æ®ç»“æ„ç†è§£ï¼š
- Annotations_6fps/video_0001/000/0.png = å¯¹è±¡000åœ¨å¸§0çš„GT
- Annotations_6fps/video_0001/001/0.png = å¯¹è±¡001åœ¨å¸§0çš„GT
- æ¯ä¸ªå¯¹è±¡åœ¨å„è‡ªçš„æ–‡ä»¶å¤¹ä¸­åŒ…å«å®Œæ•´çš„å¸§åºåˆ—
- æ‰€æœ‰å¯¹è±¡ä½¿ç”¨åŒä¸€ä¸ªJPEGImages_24fpsä¸­çš„è§†é¢‘å¸§æ¥åˆ†å‰²
"""
import os
import argparse
import numpy as np
import torch
from PIL import Image
import cv2
import sys
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sam2.build_sam import build_camsam2_video_predictor


def get_device():
    """è·å–è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    if device.type == "cuda":
        torch.autocast("cuda", dtype=torch.float32).__enter__()
        if torch.cuda.get_device_properties(0).major >= 8:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
    elif device.type == "mps":
        print("\næ³¨æ„: MPS æ”¯æŒå¤„äºè¯•éªŒé˜¶æ®µï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™")

    return device


def freeze_only_camouflaged_modules(model):
    """
    åªå†»ç»“ CamSAM2 æ–°å¢çš„æ¨¡å—ï¼ˆEOFã€OPG å’Œä¼ªè£… tokenï¼‰ï¼Œä¿æŒ SAM2+IOF æ¿€æ´»
    
    å†»ç»“çš„æ¨¡å—åŒ…æ‹¬ï¼š
    - decamouflaged_token (ä¼ªè£…token)
    - decamouflaged_mlp (ä¼ªè£…MLP)
    - OPG åŸå‹ç”Ÿæˆ (kmeans ç›¸å…³åœ¨å‰å‘ä¸­åŠ¨æ€æ‰§è¡Œï¼Œæ­¤å¤„å†»ç»“ç‰¹å¾æå–éƒ¨åˆ†)
    - EOF è¾¹ç•Œå¢å¼ºéƒ¨åˆ†
    
    ä¿æŒæ¿€æ´»çš„æ¨¡å—ï¼š
    - SAM2 ä¸»å¹² (æ‰€æœ‰å±‚)
    - IOF æ¨¡å— (compress_hiera_feat, embedding_encoder ç­‰)
    """
    # é¦–å…ˆè§£å†»æ‰€æœ‰å‚æ•°ï¼ˆè®¾ç½®ä¸ºå¯è®­ç»ƒï¼‰
    for param in model.parameters():
        param.requires_grad = True
    
    # å†»ç»“ CamSAM2 æ–°å¢æ¨¡å—
    frozen_modules = [
        'decamouflaged_token',
        'decamouflaged_mlp',
    ]
    
    frozen_count = 0
    for name, module in model.named_modules():
        # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦å†»ç»“çš„æ¨¡å—
        for frozen_module in frozen_modules:
            if frozen_module in name:
                print(f"ğŸ”’ å†»ç»“æ¨¡å—: {name}")
                for param in module.parameters():
                    param.requires_grad = False
                frozen_count += 1
                break
    
    # è®¡ç®—å†»ç»“/æ¿€æ´»å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°æ•°:     {total_params:,}")
    print(f"   æ¿€æ´»å‚æ•°:     {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")
    print(f"   å†»ç»“å‚æ•°:     {frozen_params:,} ({100*frozen_params/total_params:.2f}%)")
    print(f"\nğŸ“‹ æ¿€æ´»æ¨¡å—é…ç½®:")
    print(f"   âœ… SAM2 ä¸»å¹² (æ‰€æœ‰å±‚)")
    print(f"   âœ… IOF æ¨¡å— (compress_hiera_feat, embedding_encoder ç­‰)")
    print(f"   ğŸ”’ å†»ç»“: ä¼ªè£… token å’Œ MLP (EOF/OPG)")


def inference_single_object(video_name, object_id, annotations_path, images_path, 
                            output_path, predictor, output_mode):
    """
    å¯¹è§†é¢‘ä¸­çš„å•ä¸ªåˆ†å‰²å¯¹è±¡è¿›è¡Œæ¨ç†
    
    å‚æ•°ï¼š
        video_name: è§†é¢‘åç§°ï¼ˆå¦‚ 'video_0001'ï¼‰
        object_id: å¯¹è±¡ IDï¼ˆå¦‚ 0, 1, 2ï¼‰
        annotations_path: Annotations_6fps çš„è·¯å¾„
        images_path: JPEGImages_24fps çš„è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        predictor: SAM2/CamSAM2 é¢„æµ‹å™¨
        output_mode: è¾“å‡ºæ¨¡å¼
    
    è¿”å›ï¼š
        True å¦‚æœæˆåŠŸï¼ŒFalse å¦‚æœå‡ºé”™
    """
    try:
        # è·¯å¾„
        object_annotation_dir = os.path.join(annotations_path, video_name, f"{object_id:03d}")
        video_images_dir = os.path.join(images_path, video_name)
        
        if not os.path.exists(object_annotation_dir):
            print(f"      âš ï¸  å¯¹è±¡ç›®å½•ä¸å­˜åœ¨: {object_annotation_dir}")
            return False
        
        if not os.path.exists(video_images_dir):
            print(f"      âš ï¸  è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {video_images_dir}")
            return False
        
        # åŠ è½½è¯¥å¯¹è±¡çš„æ‰€æœ‰å¸§ GTï¼ˆç”¨äºè·å–ç¬¬ä¸€å¸§æç¤ºï¼‰
        frames_data = {}
        frame_files = sorted([f for f in os.listdir(object_annotation_dir)
                             if f.endswith('.png')],
                            key=lambda x: int(os.path.splitext(x)[0]))
        
        for frame_file in frame_files:
            frame_id = int(os.path.splitext(frame_file)[0])
            frame_mask = np.array(Image.open(os.path.join(object_annotation_dir, frame_file)))
            frames_data[frame_id] = frame_mask
        
        if not frames_data:
            print(f"      âš ï¸  å¯¹è±¡æ— æœ‰æ•ˆå¸§æ•°æ®")
            return False
        
        # è·å–è§†é¢‘ä¸­çš„æ‰€æœ‰å¸§
        frame_files = sorted([f for f in os.listdir(video_images_dir)
                             if f.endswith(('.jpg', '.jpeg', '.JPG', '.JPEG'))],
                            key=lambda x: int(os.path.splitext(x)[0]))
        total_frames = len(frame_files)
        
        # è·å–ç¬¬ä¸€å¸§çš„ mask ä½œä¸ºæç¤º
        first_frame_ids = sorted(frames_data.keys())
        first_frame_id = first_frame_ids[0]
        first_frame_mask = frames_data[first_frame_id]
        
        # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
        inference_state = predictor.init_state(video_path=video_images_dir, output_mode=output_mode)
        predictor.reset_state(inference_state)
        
        # æ·»åŠ  mask æç¤ºï¼ˆåœ¨ç¬¬ä¸€å¸§ï¼‰
        ann_obj_id = 1
        _, out_obj_ids, out_mask_logits = predictor.add_new_mask(
            inference_state=inference_state,
            frame_idx=first_frame_id,
            obj_id=ann_obj_id,
            mask=first_frame_mask
        )
        
        # æ¨ç†æ•´ä¸ªè§†é¢‘
        video_segments = {}
        for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):
            video_segments[out_frame_idx] = {
                out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()
                for i, out_obj_id in enumerate(out_obj_ids)
            }
        
        # æ”¶é›†æ¨ç†ç»“æœ
        prediction_to_eval = []
        for frame_idx in range(total_frames):
            if frame_idx in video_segments:
                for out_obj_id, out_mask in video_segments[frame_idx].items():
                    prediction_to_eval.append(out_mask[0])
                    break
            else:
                prediction_to_eval.append(np.zeros_like(first_frame_mask))
        
        prediction_to_eval = np.array(prediction_to_eval)
        
        # ä¿å­˜é¢„æµ‹ç»“æœï¼ˆPNG æ ¼å¼ï¼‰
        save_dir = os.path.join(output_path, video_name, f"{object_id:03d}")
        os.makedirs(save_dir, exist_ok=True)
        
        for i, pred_mask in enumerate(prediction_to_eval):
            save_file = os.path.join(save_dir, f"{i:05d}.png")
            cv2.imwrite(save_file, (pred_mask * 255).astype(np.uint8))
        
        print(f"      âœ… å¯¹è±¡ {object_id:03d}: å·²ä¿å­˜ {len(prediction_to_eval)} å¸§")
        return True
        
    except Exception as e:
        print(f"      âŒ å¯¹è±¡ {object_id:03d} å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def inference_video_all_objects(video_name, annotations_path, images_path, 
                                output_path, predictor, output_mode):
    """
    å¯¹è§†é¢‘ä¸­çš„æ‰€æœ‰åˆ†å‰²å¯¹è±¡è¿›è¡Œæ¨ç†
    
    è¿”å›ï¼š
        True å¦‚æœè‡³å°‘ä¸€ä¸ªå¯¹è±¡æˆåŠŸï¼ŒFalse å¦‚æœå…¨éƒ¨å¤±è´¥
    """
    print(f"\nğŸ¬ å¤„ç†è§†é¢‘: {video_name}")
    
    video_annotation_dir = os.path.join(annotations_path, video_name)
    
    if not os.path.exists(video_annotation_dir):
        print(f"   âŒ æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {video_annotation_dir}")
        return False
    
    # è·å–è¯¥è§†é¢‘ä¸­çš„æ‰€æœ‰å¯¹è±¡ ID
    object_dirs = sorted([d for d in os.listdir(video_annotation_dir) 
                         if os.path.isdir(os.path.join(video_annotation_dir, d))],
                        key=lambda x: int(x) if x.isdigit() else 999)
    object_ids = [int(obj_dir) for obj_dir in object_dirs]
    
    if not object_ids:
        print(f"   âš ï¸  æœªæ‰¾åˆ°åˆ†å‰²å¯¹è±¡")
        return False
    
    print(f"   ğŸ“Š æ‰¾åˆ° {len(object_ids)} ä¸ªåˆ†å‰²å¯¹è±¡: {object_ids}")
    
    success_count = 0
    for object_id in object_ids:
        success = inference_single_object(
            video_name=video_name,
            object_id=object_id,
            annotations_path=annotations_path,
            images_path=images_path,
            output_path=output_path,
            predictor=predictor,
            output_mode=output_mode
        )
        if success:
            success_count += 1
    
    return success_count > 0


def parse_args():
    """å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser("SAV-test è¯„ä¼°ï¼ˆSAM2+IOF é…ç½®ï¼‰", add_help=True)
    parser.add_argument("--model_cfg", type=str, default="sam2_hiera_t.yaml",
                        help="æ¨¡å‹é…ç½®æ–‡ä»¶")
    parser.add_argument("--ckpt_path", type=str, default="checkpoints/sam2_hiera_tiny.pt",
                        help="SAM2 æ¨¡å‹æƒé‡")
    parser.add_argument("--camsam2_extra", type=str, required=True,
                        help="CamSAM2 ä¼ªè£…æ¨¡å—æƒé‡ï¼ˆå¿…éœ€ï¼‰")
    parser.add_argument("--output_mode", type=str, default="combined_mask",
                        choices=["original_sam2_mask", "combined_mask"],
                        help="è¾“å‡ºæ¨¡å¼ï¼ˆcombined_mask åŒ…å« IOFï¼‰")
    parser.add_argument("--annotations_path", type=str, required=True,
                        help="Annotations_6fps è·¯å¾„")
    parser.add_argument("--images_path", type=str, required=True,
                        help="JPEGImages_24fps è·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True,
                        help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--freeze_except_iof", action="store_true", default=True,
                        help="å†»ç»“ä¼ªè£…æ¨¡å—ï¼ˆEOF/OPGï¼‰ï¼Œåªä¿æŒ SAM2+IOF æ¿€æ´»ï¼ˆé»˜è®¤ Trueï¼‰")
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    device = get_device()
    
    print(f"\n{'='*80}")
    print(f"ğŸ¯ CamSAM2 SAV-test æ¨ç†ï¼ˆSAM2+IOF é…ç½®ï¼‰")
    print(f"{'='*80}")
    print(f"âœ… SAM2 æ¨¡å‹: {args.model_cfg}")
    print(f"âœ… SAM2 æƒé‡: {args.ckpt_path}")
    print(f"âœ… CamSAM2 æƒé‡: {args.camsam2_extra}")
    print(f"âœ… è¾“å‡ºæ¨¡å¼: {args.output_mode}")
    print(f"âœ… å†»ç»“ä¼ªè£…æ¨¡å—: {args.freeze_except_iof}")
    print(f"{'='*80}\n")
    
    # æ„å»ºé¢„æµ‹å™¨
    print("ğŸ“¦ æ„å»ºæ¨¡å‹...")
    predictor = build_camsam2_video_predictor(
        args.model_cfg, 
        args.ckpt_path, 
        device=device, 
        camsam2_extra=args.camsam2_extra
    )
    
    # å†»ç»“ä¼ªè£…æ¨¡å—
    if args.freeze_except_iof and args.output_mode == "combined_mask":
        print("\nğŸ”’ é…ç½®æ¨¡å—å†»ç»“...")
        # ç›´æ¥è®¿é—® predictor çš„ sam_mask_decoder
        if hasattr(predictor, 'sam_mask_decoder'):
            freeze_only_camouflaged_modules(predictor.sam_mask_decoder)
        else:
            print("âš ï¸  æ‰¾ä¸åˆ° sam_mask_decoderï¼Œè·³è¿‡å†»ç»“")
            # å°è¯•åˆ—å‡º predictor ä¸­æ‰€æœ‰å±æ€§
            print("   å¯ç”¨å±æ€§:", [attr for attr in dir(predictor) if not attr.startswith('_')][:10])
    
    predictor.eval()
    
    # è·å–æ‰€æœ‰è§†é¢‘
    print(f"\nğŸ“‚ æ‰«ææ•°æ®é›†...")
    videos = sorted([v for v in os.listdir(args.annotations_path) 
                    if os.path.isdir(os.path.join(args.annotations_path, v))])
    
    print(f"ğŸ“¹ æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘\n")
    
    # æ¨ç†æ¯ä¸ªè§†é¢‘çš„æ‰€æœ‰å¯¹è±¡
    os.makedirs(args.output_path, exist_ok=True)
    success_count = 0
    total_count = 0
    
    for video_name in videos:
        success = inference_video_all_objects(
            video_name=video_name,
            annotations_path=args.annotations_path,
            images_path=args.images_path,
            output_path=args.output_path,
            predictor=predictor,
            output_mode=args.output_mode
        )
        if success:
            success_count += 1
        total_count += 1
    
    # ç”Ÿæˆå®ŒæˆæŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š æ¨ç†å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"âœ… æˆåŠŸå¤„ç†è§†é¢‘: {success_count}/{total_count}")
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {args.output_path}")
    print(f"âœ… æ‰€æœ‰åˆ†å‰²ç»“æœå·²ä¿å­˜ä¸º PNG æ ¼å¼")
    print("\næç¤ºï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¡ç®—æŒ‡æ ‡:")
    print(f"   python scripts/eval_jf.py --pred_dir {args.output_path} --gt_dir {args.annotations_path}")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
